{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "attack_runner_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkmEiaKkwCee"
      },
      "source": [
        "# Attack Runner Tutorial\n",
        "This notebook shows how to use the attack runner script. It's goal will be to run one MIA attack and one Direct GMIA attack and build one attack report for both attacks.\n",
        "\n",
        "Each attack configuration is based on its tutorial configuration:\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "        <td>\n",
        "        <a href=\"https://colab.research.google.com/github/hallojs/ml-pepr/blob/master/notebooks/mia_tutorial.ipynb\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" width=\"42\" height=\"42\" />MIA Tutorial</a>\n",
        "        </td>\n",
        "        <td>\n",
        "        <a href=\"https://colab.research.google.com/github/hallojs/ml-pepr/blob/master/notebooks/gmia_tutorial.ipynb\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" width=\"42\" height=\"42\" />Direct GMIA Tutorial</a>\n",
        "        </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IGAgp0n6_QU"
      },
      "source": [
        "## Prepare Environment\n",
        "**Important: Restart the Runtime after this Cell!**\n",
        "The restart is needed because of `pip install -e`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s9Dgj5SaJEo"
      },
      "source": [
        "!git clone https://github.com/hallojs/ml-pepr.git\n",
        "%pip install -e ml-pepr\n",
        "%pip install pylatex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wd0WyRD7FLx"
      },
      "source": [
        "## Imports\n",
        "Note: These are the imports needed by this notebook. If a function like a `create_model` function needs additional imports, they should be defined inside the function body. In this way the attack runner can evaluate the imports dynamically during execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNSjF3PIGWtJ"
      },
      "source": [
        "from pepr import attack_runner\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_rZeqvJ7Qsh"
      },
      "source": [
        "## Setup Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWqm9M7wGX8O"
      },
      "source": [
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# TensorFlow Logger\n",
        "file_handler_tf = logging.FileHandler('tf.log')\n",
        "file_handler_tf.setLevel(logging.INFO)\n",
        "file_handler_tf.setFormatter(formatter)\n",
        "\n",
        "tf.get_logger().setLevel(logging.INFO)\n",
        "logger_tf = tf.get_logger()\n",
        "logger_tf.addHandler(file_handler_tf)\n",
        "\n",
        "# PePR Logger\n",
        "level = logging.DEBUG\n",
        "stream_handler_pr = logging.StreamHandler()\n",
        "stream_handler_pr.setLevel(level)\n",
        "stream_handler_pr.setFormatter(formatter)\n",
        "\n",
        "# -- Add MIA logger\n",
        "file_handler_pr = logging.FileHandler('pepr.privacy.mia.log')\n",
        "file_handler_pr.setLevel(level)\n",
        "file_handler_pr.setFormatter(formatter)\n",
        "logger_pr = logging.getLogger('pepr.privacy.mia')\n",
        "logger_pr.addHandler(file_handler_pr)\n",
        "logger_pr.addHandler(stream_handler_pr)\n",
        "\n",
        "# -- Add GMIA logger\n",
        "file_handler_pr = logging.FileHandler('pepr.privacy.gmia.log')\n",
        "file_handler_pr.setLevel(level)\n",
        "file_handler_pr.setFormatter(formatter)\n",
        "logger_pr = logging.getLogger('pepr.privacy.gmia')\n",
        "logger_pr.addHandler(file_handler_pr)\n",
        "logger_pr.addHandler(stream_handler_pr)\n",
        "\n",
        "# -- Add attack runner logger\n",
        "file_handler_pr = logging.FileHandler('pepr.attack_runner.log')\n",
        "file_handler_pr.setLevel(level)\n",
        "file_handler_pr.setFormatter(formatter)\n",
        "logger_pr = logging.getLogger('pepr.attack_runner')\n",
        "logger_pr.addHandler(file_handler_pr)\n",
        "logger_pr.addHandler(stream_handler_pr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpfoMcVp7u02"
      },
      "source": [
        "## Functions\n",
        "Functions for creating models and preparing the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpSDgg7z70mD"
      },
      "source": [
        "### MIA Functions\n",
        "Define functions used by the MIA configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Yizg2MJcgj"
      },
      "source": [
        "def get_target_model(input_shape, number_of_labels):\n",
        "    target_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3,3), activation=\"tanh\", padding='same', input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPool2D((2,2)),\n",
        "        tf.keras.layers.Conv2D(64, (3,3), activation=\"tanh\", padding='same'),\n",
        "        tf.keras.layers.MaxPool2D((2,2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
        "        tf.keras.layers.Dense(number_of_labels),\n",
        "        tf.keras.layers.Softmax()\n",
        "    ])\n",
        "    return target_model\n",
        "\n",
        "def get_attack_model(number_of_labels):\n",
        "    attack_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", input_shape=(number_of_labels,)),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ])\n",
        "    return attack_model\n",
        "\n",
        "def create_compile_shadow_model():\n",
        "    \"\"\"Create compiled target/shadow model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tensorflow.python.keras.engine.sequential.Sequential\n",
        "        A compiled tensorflow model.\n",
        "    \"\"\"\n",
        "\n",
        "    from tensorflow.keras import models\n",
        "    from tensorflow.keras import optimizers\n",
        "\n",
        "    input_shape = (32, 32, 3)\n",
        "    number_classes = 100\n",
        "\n",
        "    model = get_target_model(input_shape, number_classes)\n",
        "\n",
        "    optimizer = optimizers.Adam(lr=0.0001)\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "    metrics = [\"accuracy\"]\n",
        "    model.compile(optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_compile_attack_model():\n",
        "    \"\"\"Create compiled attack model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tensorflow.python.keras.engine.sequential.Sequential\n",
        "        A compiled tensorflow model.\n",
        "    \"\"\"\n",
        "\n",
        "    from tensorflow.keras import models\n",
        "    from tensorflow.keras import optimizers\n",
        "\n",
        "    number_classes = 100\n",
        "\n",
        "    model = get_attack_model(number_classes)\n",
        "\n",
        "    optimizer = optimizers.Adam(lr=0.0001)\n",
        "    loss = 'binary_crossentropy'\n",
        "    metrics = [\"accuracy\"]\n",
        "    model.compile(optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_cifar100():\n",
        "    \"\"\"Loads and preprocesses the CIFAR100 dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        (training data, training labels, test data, test labels)\n",
        "    \"\"\"\n",
        "    train, test = tf.keras.datasets.cifar100.load_data()\n",
        "    train_data, train_labels = train\n",
        "    test_data, test_labels = test\n",
        "\n",
        "    # Normalize the data to a range between 0 and 1\n",
        "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "    # Reshape the images to (32, 32, 3)\n",
        "    train_data = train_data.reshape(train_data.shape[0], 32, 32, 3)\n",
        "    test_data = test_data.reshape(test_data.shape[0], 32, 32, 3)\n",
        "\n",
        "    train_labels = np.reshape(np.array(train_labels, dtype=np.int32), (train_labels.shape[0],))\n",
        "    test_labels = np.reshape(np.array(test_labels, dtype=np.int32), (test_labels.shape[0],))\n",
        "\n",
        "    return np.vstack((train_data, test_data)), np.hstack((train_labels, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3R7akEF8CPy"
      },
      "source": [
        "### Direct GMIA Functions\n",
        "Define functions used by the GMIA configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLesQNJzOLou"
      },
      "source": [
        "def create_model(input_shape, n_categories):\n",
        "    \"\"\"Architecture of the target and reference models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : tuple\n",
        "        Dimensions of the input for the target/training\n",
        "    n_categories : int\n",
        "        number of categories for the prediction\n",
        "    models.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tensorflow.python.keras.engine.sequential.Sequential\n",
        "        A convolutional neuronal network model.\n",
        "    \"\"\"\n",
        "\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import MaxPooling2D\n",
        "    from tensorflow.keras.layers import Conv2D\n",
        "    from tensorflow.keras.layers import Activation\n",
        "    from tensorflow.keras.layers import Dropout\n",
        "    from tensorflow.keras.layers import Flatten\n",
        "    from tensorflow.keras.layers import Dense\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # first convolution layer\n",
        "    model.add(Conv2D(filters=32, kernel_size=(5, 5), strides=(\n",
        "        1, 1), padding='same', input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # max pooling layer\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # second convolution layer\n",
        "    model.add(Conv2D(filters=64, kernel_size=(\n",
        "        5, 5), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # max pooling layer\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    # fully connected layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # drop out\n",
        "    model.add(Dropout(rate=0.5))\n",
        "\n",
        "    # fully connected layer\n",
        "    model.add(Dense(n_categories))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_compile_model():\n",
        "    \"\"\"Create compiled model.\n",
        "\n",
        "    At the moment pepr.gmia needs this function to train the reference models.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tensorflow.python.keras.engine.sequential.Sequential\n",
        "        A compiled tensorflow model.\n",
        "    \"\"\"\n",
        "\n",
        "    from tensorflow.keras import models\n",
        "    from tensorflow.keras import optimizers\n",
        "\n",
        "    input_shape = (28, 28, 1)\n",
        "    number_classes = 10\n",
        "\n",
        "    model = create_model(input_shape, number_classes)\n",
        "\n",
        "    optimizer = optimizers.Adam(lr=0.0001)\n",
        "    loss = 'categorical_crossentropy'\n",
        "    metrics = [\"accuracy\"]\n",
        "    model.compile(optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_fashion_mnist():\n",
        "    \"\"\"Loads and preprocesses the fashion mnist dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        (training data, training labels, test data, test labels)\n",
        "    \"\"\"\n",
        "\n",
        "    train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    train_data, train_labels = train\n",
        "    test_data, test_labels = test\n",
        "\n",
        "    # Normalize the data to a range between 0 and 1\n",
        "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "    # Reshape the images to (28, 28, 1)\n",
        "    train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "    test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "    train_labels = np.array(train_labels, dtype=np.int32)\n",
        "    test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "    return np.vstack((train_data, test_data)), np.hstack((train_labels, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-s-Yy1z8O12"
      },
      "source": [
        "## Train Target Models\n",
        "Train and save the target models for the attacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxMFsyLS3CD"
      },
      "source": [
        "data_c100, labels_c100 = load_cifar100()\n",
        "target_model = create_compile_shadow_model()\n",
        "target_model.fit(data_c100[40000:50000],\n",
        "                 labels_c100[40000:50000],\n",
        "                 epochs=1,\n",
        "                 batch_size=50,\n",
        "                 verbose=0)\n",
        "target_model.save('data/target_model_mia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fx-3zDUTQEX"
      },
      "source": [
        "data_fmnist, labels_fmnist = load_fashion_mnist()\n",
        "target_model = create_compile_model()\n",
        "target_model.fit(data_fmnist[40000:50000],\n",
        "                 tf.keras.utils.to_categorical(labels_fmnist[40000:50000], num_classes=10),\n",
        "                 epochs=1,\n",
        "                 batch_size=50,\n",
        "                 verbose=0)\n",
        "target_model.save('data/target_model_gmia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRALSj488YdG"
      },
      "source": [
        "## Attack Runner Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmAd-7_J9Fm8"
      },
      "source": [
        "### YAML Configuration file\n",
        "The attack runner configuration is described in an YAML file. The attack runner will parse it, load stored arrays and execute all specified attacks. More information on how to write a configuration file can be found in the [documentation](https://hallojs.github.io/ml-pepr/attack_runner.html).\n",
        "\n",
        "Our attack runner configuration for this example looks like this:\n",
        "\n",
        "```yaml\n",
        "# Attack Parameters\n",
        "attack_pars:\n",
        "  - attack_type: \"mia\"\n",
        "    attack_alias: \"MIA Tutorial\"\n",
        "    number_shadow_models: 100\n",
        "    shadow_training_set_size: 2500\n",
        "    path_to_dataset_data: \"datasets/cifar100_data.npy\"\n",
        "    path_to_dataset_labels: \"datasets/cifar100_labels.npy\"\n",
        "    number_classes: 100\n",
        "    <fn>create_compile_shadow_model: \"create_compile_shadow_model\"\n",
        "    shadow_epochs: 100\n",
        "    shadow_batch_size: 50\n",
        "    <fn>create_compile_attack_model: \"create_compile_attack_model\"\n",
        "    attack_epochs: 50\n",
        "    attack_batch_size: 50\n",
        "    target_model_paths:\n",
        "    - \"data/target_model_mia\"\n",
        "  - attack_type: \"gmia\"\n",
        "    attack_alias: \"GMIA Tutorial\"\n",
        "    number_reference_models: 100\n",
        "    reference_training_set_size: 10000\n",
        "    path_to_dataset_data: \"datasets/fmnist_data.npy\"\n",
        "    path_to_dataset_labels: \"datasets/fmnist_labels.npy\"\n",
        "    number_classes: 10\n",
        "    <fn>create_compile_model: \"create_compile_reference_model\"\n",
        "    reference_epochs: 50\n",
        "    reference_batch_size: 50\n",
        "    hlf_metric: \"cosine\"\n",
        "    hlf_layer_number: 10\n",
        "    number_target_records: 25\n",
        "    target_model_paths:\n",
        "    - \"data/target_model_gmia\"\n",
        "\n",
        "# Data Configuration\n",
        "data_conf:\n",
        "  - <np>shadow_indices: \"datasets/shadow_ref_indices.npy\"\n",
        "    <np>target_indices: \"datasets/target_indices.npy\"\n",
        "    <np>evaluation_indices: \"datasets/evaluation_indices.npy\"\n",
        "    <np>record_indices_per_target: \"datasets/record_indices_per_target.npy\"\n",
        "  - <np>reference_indices: \"datasets/shadow_ref_indices.npy\"\n",
        "    <np>target_indices: \"datasets/target_indices.npy\"\n",
        "    <np>evaluation_indices: \"datasets/evaluation_indices.npy\"\n",
        "    <np>record_indices_per_target: \"datasets/record_indices_per_target.npy\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9aT0-Zz8pjC"
      },
      "source": [
        "### Save datasets\n",
        "Save arrays that are referenced by the attack runner configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urOr4Uypv922"
      },
      "source": [
        "!mkdir -p datasets\n",
        "np.save(\"datasets/shadow_ref_indices\", np.arange(40000))\n",
        "np.save(\"datasets/target_indices\", np.arange(40000, 50000))\n",
        "np.save(\"datasets/evaluation_indices\", np.arange(40000, 60000))\n",
        "np.save(\"datasets/record_indices_per_target\", np.array([np.arange(10000)]))\n",
        "\n",
        "np.save(\"datasets/cifar100_data\", data_c100)\n",
        "np.save(\"datasets/cifar100_labels\", labels_c100)\n",
        "\n",
        "np.save(\"datasets/fmnist_data\", data_fmnist)\n",
        "np.save(\"datasets/fmnist_labels\", labels_fmnist)\n",
        "\n",
        "# Optional: Free memory\n",
        "del target_model, data_c100, labels_c100, data_fmnist, labels_fmnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woaPI2P28Apa"
      },
      "source": [
        "### Function Map\n",
        "The attack runner can only access functions from the notebook, if it knows the function pointers. The pointers are passed by a dictionary where the keys are the names which the attack runner configuration refers to (for example: `create_compile_model: \"create_compile_reference_model\"` is resolved to `create_compile_model: create_compile_model` with the function mapping below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNaxHDh_QC0P"
      },
      "source": [
        "functions = {\n",
        "    \"create_compile_reference_model\": create_compile_model,\n",
        "    \"create_compile_shadow_model\": create_compile_shadow_model,\n",
        "    \"create_compile_attack_model\": create_compile_attack_model,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHqRmcKN8Apa"
      },
      "source": [
        "## Run Attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSjMa8s54tIk"
      },
      "source": [
        "attack_paths = attack_runner.run_attacks(\"ml-pepr/notebooks/attack_runner_tutorial/attack_runner_config.yml\",\n",
        "                          \"attack_objects\", \n",
        "                          functions)\n",
        "\n",
        "attack_runner.create_report(attack_paths, \"report\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zOifysHzgsj"
      },
      "source": [
        "# Zip report directory if you want to download it from google colab\n",
        "!zip -r -q report.zip report"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}